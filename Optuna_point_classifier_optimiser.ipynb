{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This script is used to obtain an optimised network architecture. It uses Optuna to minimize the objective function.\n"
      ],
      "metadata": {
        "id": "1IzaPau-yEc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet optuna"
      ],
      "metadata": {
        "id": "32HY170rVrUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaaFSb9KVD1F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "mBMqJaRLVPuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "n_trials = 500 # number of trials to perform\n",
        "dimension = 2 # dimension of dataset\n",
        "noise = False # add noise to the dataset\n",
        "dataset_type = 'square' # 'circle' or 'square'\n",
        "N = 3200 # number of points in the dataset\n",
        "num_epochs = 100 # number of epochs before training is stopped\n",
        "batch_size = 32\n",
        "learning_rate = 0.001 # learning rate for training\n",
        "slope = 0.01 # slope for Leaky functions\n",
        "patience = 6"
      ],
      "metadata": {
        "id": "6bX9ZiyVXSyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHspPPbvngJc"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "\n",
        "def sumOne(u):\n",
        "  u = Positive(u)\n",
        "  u = u/torch.sum(u)\n",
        "  return Positive(u)\n",
        "\n",
        "def Positive(X):\n",
        "    return torch.abs(X)\n",
        "\n",
        "# From https://github.com/JiJingYu/delta_orthogonal_init_pytorch/blob/master/demo.py\n",
        "def genOrthgonal(dim):\n",
        "    a = torch.zeros((dim, dim)).normal_(0, 1)\n",
        "    q, r = torch.linalg.qr(a)\n",
        "    d = torch.diag(r, 0).sign()\n",
        "    diag_size = d.size(0)\n",
        "    d_exp = d.view(1, diag_size).expand(diag_size, diag_size)\n",
        "    q.mul_(d_exp)\n",
        "    return q\n",
        "\n",
        "def makeDeltaOrthogonal(weights, gain):\n",
        "    rows = weights.size(0)\n",
        "    cols = weights.size(1)\n",
        "    if rows > cols:\n",
        "        print(\"In_filters should not be greater than out_filters.\")\n",
        "    weights.data.fill_(0)\n",
        "    dim = max(rows, cols)\n",
        "    q = genOrthgonal(dim)\n",
        "    # mid1 = weights.size(2) // 2\n",
        "    # mid2 = weights.size(3) // 2\n",
        "    # weights[:, :, mid1, mid2] = q[:weights.size(0), :weights.size(1)]\n",
        "    weigths = q\n",
        "    weights.mul_(gain)\n",
        "\n",
        "#Convolutional layers\n",
        "def deconv_orth_dist(kernel, padding = 2, stride = 1):\n",
        "    [o_c, i_c, w, h] = kernel.shape\n",
        "    output = torch.conv2d(kernel, kernel, padding=padding)\n",
        "    target = torch.zeros((o_c, o_c, output.shape[-2], output.shape[-1])).to(kernel.device)\n",
        "    ct = int(np.floor(output.shape[-1]/2))\n",
        "    target[:,:,ct,ct] = torch.eye(o_c).to(kernel.device)\n",
        "    return torch.norm( output - target )\n",
        "\n",
        "def deconv_orth_dist_2d(matrix):\n",
        "    [w, h] = matrix.shape\n",
        "    target = torch.eye(w)\n",
        "    # print(f'Matrix: {matrix}')\n",
        "    # print(f'Target: {target}')\n",
        "    return torch.linalg.matrix_norm(matrix-target,2)\n",
        "\n",
        "\n",
        "def conv_orth_dist(kernel, stride = 1):\n",
        "    [o_c, i_c, w, h] = kernel.shape\n",
        "    assert (w == h),\"Do not support rectangular kernel\"\n",
        "    assert stride<w,\"Please use matrix orthgonality instead\"\n",
        "    new_s = stride*(w-1) + w#np.int(2*(half+np.floor(half/stride))+1)\n",
        "    temp = torch.eye(new_s*new_s*i_c).reshape((new_s*new_s*i_c, i_c, new_s,new_s)).to(kernel.device)\n",
        "    out = (F.conv2d(temp, kernel, stride=stride)).reshape((new_s*new_s*i_c, -1))\n",
        "    Vmat = out[np.floor(new_s**2/2).astype(int)::new_s**2, :]\n",
        "    temp= np.zeros((i_c, i_c*new_s**2))\n",
        "    for i in range(temp.shape[0]):temp[i,np.floor(new_s**2/2).astype(int)+new_s**2*i]=1\n",
        "    return torch.norm( Vmat@torch.t(out) - torch.from_numpy(temp).float().to(kernel.device) )\n",
        "\n",
        "#Fully connected layers\n",
        "def orth_dist(mat, stride=None):\n",
        "    mat = mat.reshape( (mat.shape[0], -1) )\n",
        "    if mat.shape[0] < mat.shape[1]:\n",
        "        mat = mat.permute(1,0)\n",
        "    return torch.norm( torch.t(mat)@mat - torch.eye(mat.shape[1]).to(mat.device))\n",
        "\n",
        "def power_method(A, A_t, u_init, k=1):\n",
        "    u = u_init\n",
        "    for i in range(k):\n",
        "        v = A(u)\n",
        "        v /= torch.sqrt(torch.sum(v**2))\n",
        "        u = A_t(v)\n",
        "        sigma = torch.sum(u * u)\n",
        "        u /= torch.sqrt(torch.sum(u**2))\n",
        "    return sigma, u[0] #so it returns a 3d tensor\n",
        "\n",
        "def compute_spectral_norm(conv, u_init=None, im_size=(3, 32, 32), k=1):\n",
        "    if u_init is None:\n",
        "        with torch.no_grad():\n",
        "            u_init = torch.randn(1, *im_size).to(conv.weight.device)\n",
        "    u_init = u_init.to(conv.weight.device)\n",
        "    with torch.no_grad():\n",
        "        return power_method(lambda u: torch.nn.functional.conv2d(u, conv.weight, padding=tuple(v//2 for v in conv.weight.shape[2:])),\n",
        "                lambda v: torch.nn.functional.conv_transpose2d(v, conv.weight, padding=tuple(v//2 for v in conv.weight.shape[2:])),\n",
        "                u_init, k)\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    convo = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1,bias=False)\n",
        "    layers = [convo]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class multiClassHingeLoss(nn.Module): # From paper Dynamical Systems\n",
        "    def __init__(self, p=1, margin=0.1, device='cpu', size_average=True):\n",
        "        super(multiClassHingeLoss, self).__init__()\n",
        "        self.margin=margin\n",
        "        self.size_average=size_average\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, output, y):\n",
        "        output_y=output[torch.arange(0,y.size()[0]).long().to(self.device),y.data.to(self.device)].view(-1,1) #it is a (Batch Size x 1) tensor, having entries that are x[y]\n",
        "        loss=output-output_y+self.margin #this has self.margin in position y and the difference between the entry of x and x[y] in the other positions\n",
        "        #remove i=y items\n",
        "        loss[torch.arange(0,y.size()[0]).long().to(self.device),y.data.to(self.device)]=0 #sets to 0 the entry in position y, instead of having self.margin\n",
        "        #max(0,_)\n",
        "        loss[loss<0]=0 #sets to 0 the entries of loss where we have negative numbers, i.e. those meeting the margin (there is a higher difference than the margin between x[y] and x[i])\n",
        "        #sum up\n",
        "        loss=torch.sum(loss)\n",
        "        if(self.size_average):\n",
        "            loss/=output.size()[0]\n",
        "        return loss"
      ],
      "metadata": {
        "id": "xF43GC4jpMe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions used to generate datasets\n",
        "num_points = N\n",
        "def GenerateCircleDataset(noise=False):\n",
        "  # Generate random points\n",
        "  points = np.random.randn(num_points, dimension)\n",
        "\n",
        "  # Calculate distances to the origin\n",
        "  distances = np.linalg.norm(points, axis=1)\n",
        "\n",
        "  # Generate labels: 0 if distance < 2/3, 1 if 2/3 < distance < 4/3, 2 if distance > 4/3\n",
        "  labels = np.zeros_like(distances)\n",
        "  labels[distances > 2/3] = 1\n",
        "  labels[distances > 4/3] = 2\n",
        "\n",
        "  # Convert numpy arrays to PyTorch tensors\n",
        "  labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "  if noise:\n",
        "    noise = 0.1*np.random.randn(num_points, dimension)\n",
        "    noisy_points = points + noise\n",
        "    points_tensor = torch.tensor(noisy_points, dtype=torch.float32)\n",
        "  else:\n",
        "    points_tensor = torch.tensor(points, dtype=torch.float32)\n",
        "\n",
        "  return points_tensor, labels_tensor\n",
        "\n",
        "def GenerateSquareDataset(noise=False):\n",
        "    # Generate random points uniformly between -1 and 1\n",
        "    points = np.random.uniform(low=-1.0, high=1.0, size=(num_points, dimension))\n",
        "\n",
        "    # Initialize labels\n",
        "    labels = np.zeros(num_points)\n",
        "\n",
        "    # Assign labels based on quadrant\n",
        "    for i, point in enumerate(points):\n",
        "        if point[0] >= 0 and point[1] >= 0:\n",
        "            labels[i] = 0  # Quadrant 1\n",
        "        elif point[0] < 0 and point[1] >= 0:\n",
        "            labels[i] = 1  # Quadrant 2\n",
        "        elif point[0] < 0 and point[1] < 0:\n",
        "            labels[i] = 2  # Quadrant 3\n",
        "        else:\n",
        "            labels[i] = 3  # Quadrant 4\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    if noise:\n",
        "        # Add noise to the points\n",
        "        noise = 0.1 * np.random.randn(num_points, dimension)\n",
        "        noisy_points = points + noise\n",
        "        points_tensor = torch.tensor(noisy_points, dtype=torch.float32)\n",
        "    else:\n",
        "        points_tensor = torch.tensor(points, dtype=torch.float32)\n",
        "\n",
        "    return points_tensor, labels_tensor"
      ],
      "metadata": {
        "id": "7G9Oeik1ENOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training data\n",
        "\n",
        "if dataset_type == 'circle':\n",
        "  points_tensor, labels_tensor = GenerateCircleDataset(noise=noise)\n",
        "elif dataset_type == 'square':\n",
        "  points_tensor, labels_tensor = GenerateSquareDataset(noise=noise)\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.axis('equal')\n",
        "plt.scatter(points_tensor[:,0], points_tensor[:,1], c=labels_tensor)\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Random Points')\n",
        "plt.xlabel('X axis')\n",
        "plt.ylabel('Y axis')\n",
        "\n",
        "\n",
        "# Generate validation data\n",
        "if dataset_type == 'circle':\n",
        "  val_points_tensor, val_labels_tensor = GenerateCircleDataset(noise=False) # no noise on validation data\n",
        "elif dataset_type == 'square':\n",
        "  test_points_tensor, test_labels_tensor = GenerateSquareDataset(noise=False)\n"
      ],
      "metadata": {
        "id": "FE5JuhNMErVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset_type == 'circle':\n",
        "    num_outputs = 3\n",
        "elif dataset_type == 'square':\n",
        "    num_outputs = 4\n",
        "else:\n",
        "    raise ValueError(\"Invalid dataset type\")\n",
        "\n",
        "class DynamicModel(nn.Module): # Fully connected model\n",
        "    def __init__(self, trial):\n",
        "        super(DynamicModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        n_layers = trial.suggest_int('n_layers', 2, 5)  # Suggesting between 2 and 5 layers\n",
        "\n",
        "        in_features = dimension\n",
        "        for i in range(n_layers):\n",
        "            out_features = trial.suggest_int(f'n_units_l{i}', 2, 128)\n",
        "            self.layers.append(nn.Linear(in_features, out_features))\n",
        "            in_features = out_features\n",
        "\n",
        "        self.layers.append(nn.Linear(out_features, num_outputs))  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x\n",
        "\n",
        "def objective(trial):\n",
        "    # Create a dynamic model based on the trial's suggestions\n",
        "    model = DynamicModel(trial).to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    criterion = multiClassHingeLoss(margin=margin)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i in range(0, N, batch_size):\n",
        "          inputs = points_tensor[i:i+batch_size]\n",
        "          targets = labels_tensor[i:i+batch_size]\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, N, batch_size):\n",
        "            batch_features = val_points_tensor[i:i+batch_size]\n",
        "            batch_labels = val_labels_tensor[i:i+batch_size]\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= (N // batch_size)  # Average validation loss over batches\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "class ExpansiveContractiveNet(nn.Module):\n",
        "    def __init__(self, trial, nl, in_channels=dimension, sub_steps=1):\n",
        "        super(ExpansiveContractiveNet, self).__init__()\n",
        "        n_blocks = trial.suggest_int('n_blocks', 1, 20)\n",
        "        self.nlayers = n_blocks*2 # how many total layers? = blocks * 2\n",
        "        self.nl = nl  # activation function\n",
        "        self.slope = 0.01\n",
        "        self.sub_steps = sub_steps\n",
        "        self.u = torch.nn.Parameter(torch.rand(self.nlayers))\n",
        "        self.rescalings = nn.Parameter(torch.rand(1+self.nlayers//2))\n",
        "\n",
        "        # Layers\n",
        "        self.nodes_per_layer = trial.suggest_int('nodes_per_layer', 2, 64)\n",
        "\n",
        "        self.lift = nn.Linear(in_channels, self.nodes_per_layer)\n",
        "\n",
        "        self.inner_layers = nn.ModuleList([nn.Linear(self.nodes_per_layer, self.nodes_per_layer) for i in range(self.nlayers)])\n",
        "\n",
        "        self.project = nn.Linear(self.nodes_per_layer, 4)\n",
        "\n",
        "        self.rescalings = nn.Parameter(torch.rand(1+self.nlayers//2))\n",
        "\n",
        "    def getDepth(self):\n",
        "        return self.nlayers\n",
        "\n",
        "    def getReg(self,):\n",
        "        reg = 0\n",
        "        for i in range(self.nlayers):\n",
        "            reg += deconv_orth_dist_2d(self.inner_layers[i].weight)\n",
        "        return reg\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.rescalings.data[:-1] = torch.clip(self.rescalings.data[:-1],min=0,max=1)\n",
        "        x = torch.relu(self.rescalings[-1] * self.lift(x))\n",
        "\n",
        "        dts = Positive(self.u)\n",
        "\n",
        "        for i in np.arange(0,self.nlayers,2):\n",
        "\n",
        "          dte = dts[i]/self.sub_steps\n",
        "          dtc = dts[i+1]/self.sub_steps\n",
        "          Ae = self.inner_layers[i]\n",
        "          Ac = self.inner_layers[i+1]\n",
        "\n",
        "          for k in range(self.sub_steps):\n",
        "                x = x + dte * self.rescalings[i//2] * self.nl(Ae(x), self.slope)\n",
        "                x = x - dtc * self.nl(Ac(x), self.slope)\n",
        "\n",
        "        x = self.project(x) # project to number of labels\n",
        "        return x\n",
        "\n",
        "def objective_lipschitz_constrained(trial):\n",
        "    # Create a dynamic model based on the trial's suggestions\n",
        "    # model = DynamicModel(trial).to(device)\n",
        "    model = ExpansiveContractiveNet(trial, nl=F.relu).to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    margin = 0.0013\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    criterion = multiClassHingeLoss(margin=margin)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "    train_points, val_points, train_labels, val_labels = train_test_split(points_tensor, labels_tensor, test_size=0.2)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    gamma = .1 # used to scale the orthogonal constraint regulaiton\n",
        "\n",
        "    for epoch in range(1000):\n",
        "        model.train()\n",
        "        for i in range(0, len(train_points), batch_size):\n",
        "          inputs = train_points[i:i+batch_size]\n",
        "          targets = train_labels[i:i+batch_size]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          if True:\n",
        "                loss += gamma * model.getReg()\n",
        "          if math.isnan(loss.item()) or math.isinf(loss.item()):\n",
        "              print(\"NaN loss detected, stopping training.\")\n",
        "              return -1\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if True:\n",
        "                with torch.no_grad():\n",
        "                    refLip = 1\n",
        "\n",
        "                    lift_norm = torch.linalg.matrix_norm(model.lift.weight.data,2)\n",
        "                    model.lift.weight.data /= max(1,lift_norm/refLip)\n",
        "\n",
        "                for q in range(model.getDepth()):\n",
        "                  norm = torch.linalg.matrix_norm(model.inner_layers[q].weight.data,2)\n",
        "                  model.inner_layers[q].weight.data /= max(1,norm/refLip)\n",
        "\n",
        "        # Compute accuracy on test set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            norms = []\n",
        "            nonort = []\n",
        "            lift = []\n",
        "            liftOrt = []\n",
        "\n",
        "            n = torch.linalg.matrix_norm(model.lift.weight.data,2)\n",
        "            lift.append(n.item())\n",
        "            for s in range(model.getDepth()):\n",
        "              n = torch.linalg.matrix_norm(model.inner_layers[s].weight.data,2)\n",
        "              nonort.append(deconv_orth_dist_2d(model.inner_layers[s].weight).item())\n",
        "            outputs = model(test_points_tensor)\n",
        "            _, predicted = torch.max(outputs, 1) # Maximum along axis=1\n",
        "            accuracy = (predicted == test_labels_tensor).float().mean()\n",
        "\n",
        "            outputs = model(test_points_tensor)\n",
        "            loss = criterion(outputs, test_labels_tensor)\n",
        "            test_loss = loss.item()\n",
        "\n",
        "        # Validation step\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(val_points), batch_size):\n",
        "                inputs = val_points[i:i+batch_size]\n",
        "                targets = val_labels[i:i+batch_size]\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                if torch.isnan(loss) or math.isinf(loss.item()):\n",
        "                  print(\"NaN loss detected, stopping training.\")\n",
        "                  return -1, -1, epoch\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_points) // batch_size\n",
        "        # print(f'Time: {datetime.now().strftime(\"%H:%M:%S\")}, Epoch [{epoch+1}/{1000}], Loss: {loss.item()}, Val Loss: {val_loss}, Epochs without improvement:{epochs_without_improvement}')\n",
        "\n",
        "        # Check for early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "                break\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "ueS4VrJulorX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective_lipschitz_constrained, n_trials=n_trials)\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "xUMqKU_rtJif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "FjPdcFF5cKBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_slice(study)"
      ],
      "metadata": {
        "id": "-rzmSJjgcNrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_contour(study)"
      ],
      "metadata": {
        "id": "Ivlj6exMcZwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract trial values\n",
        "values = [trial.value for trial in study.trials]\n",
        "\n",
        "# Plot using matplotlib with a logarithmic scale\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(values, marker='o')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Trial')\n",
        "plt.ylabel('Objective Value')\n",
        "plt.title('Optimization History (Logarithmic Scale)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vZbnq8VzxzBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the minimum value observed up to each trial\n",
        "min_values = [min(values[:i+1]) for i in range(len(values))]\n",
        "\n",
        "# Plot using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(values)), values, marker='.', c='blue', label='Trial values')\n",
        "plt.plot(range(len(values)), values, linestyle='', color='blue', alpha=0.5)\n",
        "plt.plot(range(len(min_values)), min_values, linestyle='-', color='red', label='Minimum loss value encountered in previous trials')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Trial')\n",
        "plt.ylabel('Objective Value')\n",
        "plt.title('Optimization History')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCpvsfEK5YcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
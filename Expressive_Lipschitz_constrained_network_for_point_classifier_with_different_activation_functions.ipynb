{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FydnGVNXyU6t"
      },
      "source": [
        "This script is used to perform experiments with an expressive Lipschitz-constrained network architecture. Takes about 15 minutes for 10 loops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Jk0IRymHsK"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet foolbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaaFSb9KVD1F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import foolbox as fb\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBMqJaRLVPuj"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kSPC10ad6GK"
      },
      "outputs": [],
      "source": [
        "dimension = 2 # dimension of dataset\n",
        "noise = False # add noise to the dataset\n",
        "dataset_type = 'square' # 'circle' or 'square'\n",
        "num_points = 3200 # number of points in the training dataset\n",
        "num_test_points = 620 # number of points in the test dataset\n",
        "epochs = 1000 # number of epochs before training is stopped\n",
        "num_loops = 10 # number of times that each experiment is repeated\n",
        "batch_size = 32\n",
        "max_epsilon = 1.2\n",
        "number_of_epsilons = int(max_epsilon * 40) # number of different attack sizes that are tested\n",
        "epsilons = np.linspace(0.0, max_epsilon, number_of_epsilons) # relative attack sizes\n",
        "learning_rate = 0.001 # learning rate for training\n",
        "weight_decay = 1e-5 # weight decay for training\n",
        "patience = 6 # number of epochs to wait for validation error to improve before early stopping\n",
        "\n",
        "nodes_per_layer = 8\n",
        "# constrain = True\n",
        "# reg = True\n",
        "slope = 0.01\n",
        "blocks = 1\n",
        "inner_layers = blocks*2\n",
        "margin = 0.0013232710934127376\n",
        "\n",
        "\n",
        "# Models\n",
        "model_names = [\n",
        "    'ReLU', 'Leaky ReLU', 'Softplus',\n",
        "    'Leaky Softplus', 'Semi-Leaky Softplus','Tanh',\n",
        "    'Leaky Tanh', 'Sigmoid', 'Leaky Sigmoid'\n",
        "]\n",
        "\n",
        "n_models = len(model_names)\n",
        "\n",
        "# Corresponding colors for each model\n",
        "colors = [\n",
        "    'blue', 'lightblue', 'darkorange', 'orange', 'gold', 'green',\n",
        "    'lightgreen', 'red', 'pink'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHspPPbvngJc"
      },
      "outputs": [],
      "source": [
        "# Utils, code from the paper Dynamical Systems\n",
        "\n",
        "def sumOne(u):\n",
        "  u = Positive(u)\n",
        "  u = u/torch.sum(u)\n",
        "  return Positive(u)\n",
        "\n",
        "def Positive(X):\n",
        "    return torch.abs(X)\n",
        "\n",
        "# From https://github.com/JiJingYu/delta_orthogonal_init_pytorch/blob/master/demo.py\n",
        "def genOrthgonal(dim):\n",
        "    a = torch.zeros((dim, dim)).normal_(0, 1)\n",
        "    q, r = torch.linalg.qr(a)\n",
        "    d = torch.diag(r, 0).sign()\n",
        "    diag_size = d.size(0)\n",
        "    d_exp = d.view(1, diag_size).expand(diag_size, diag_size)\n",
        "    q.mul_(d_exp)\n",
        "    return q\n",
        "\n",
        "def makeDeltaOrthogonal(weights, gain):\n",
        "    rows = weights.size(0)\n",
        "    cols = weights.size(1)\n",
        "    if rows > cols:\n",
        "        print(\"In_filters should not be greater than out_filters.\")\n",
        "    weights.data.fill_(0)\n",
        "    dim = max(rows, cols)\n",
        "    q = genOrthgonal(dim)\n",
        "    # mid1 = weights.size(2) // 2\n",
        "    # mid2 = weights.size(3) // 2\n",
        "    # weights[:, :, mid1, mid2] = q[:weights.size(0), :weights.size(1)]\n",
        "    weigths = q\n",
        "    weights.mul_(gain)\n",
        "\n",
        "#Convolutional layers\n",
        "def deconv_orth_dist(kernel, padding = 2, stride = 1):\n",
        "    [o_c, i_c, w, h] = kernel.shape\n",
        "    output = torch.conv2d(kernel, kernel, padding=padding)\n",
        "    target = torch.zeros((o_c, o_c, output.shape[-2], output.shape[-1])).to(kernel.device)\n",
        "    ct = int(np.floor(output.shape[-1]/2))\n",
        "    target[:,:,ct,ct] = torch.eye(o_c).to(kernel.device)\n",
        "    return torch.norm( output - target )\n",
        "\n",
        "def deconv_orth_dist_2d(matrix):\n",
        "    [w, h] = matrix.shape\n",
        "    target = torch.eye(w)\n",
        "    # print(f'Matrix: {matrix}')\n",
        "    # print(f'Target: {target}')\n",
        "    return torch.linalg.matrix_norm(matrix-target,2)\n",
        "\n",
        "\n",
        "def conv_orth_dist(kernel, stride = 1):\n",
        "    [o_c, i_c, w, h] = kernel.shape\n",
        "    assert (w == h),\"Do not support rectangular kernel\"\n",
        "    assert stride<w,\"Please use matrix orthgonality instead\"\n",
        "    new_s = stride*(w-1) + w#np.int(2*(half+np.floor(half/stride))+1)\n",
        "    temp = torch.eye(new_s*new_s*i_c).reshape((new_s*new_s*i_c, i_c, new_s,new_s)).to(kernel.device)\n",
        "    out = (F.conv2d(temp, kernel, stride=stride)).reshape((new_s*new_s*i_c, -1))\n",
        "    Vmat = out[np.floor(new_s**2/2).astype(int)::new_s**2, :]\n",
        "    temp= np.zeros((i_c, i_c*new_s**2))\n",
        "    for i in range(temp.shape[0]):temp[i,np.floor(new_s**2/2).astype(int)+new_s**2*i]=1\n",
        "    return torch.norm( Vmat@torch.t(out) - torch.from_numpy(temp).float().to(kernel.device) )\n",
        "\n",
        "#Fully connected layers\n",
        "def orth_dist(mat, stride=None):\n",
        "    mat = mat.reshape( (mat.shape[0], -1) )\n",
        "    if mat.shape[0] < mat.shape[1]:\n",
        "        mat = mat.permute(1,0)\n",
        "    return torch.norm( torch.t(mat)@mat - torch.eye(mat.shape[1]).to(mat.device))\n",
        "\n",
        "def power_method(A, A_t, u_init, k=1):\n",
        "    u = u_init\n",
        "    for i in range(k):\n",
        "        v = A(u)\n",
        "        v /= torch.sqrt(torch.sum(v**2))\n",
        "        u = A_t(v)\n",
        "        sigma = torch.sum(u * u)\n",
        "        u /= torch.sqrt(torch.sum(u**2))\n",
        "    return sigma, u[0] #so it returns a 3d tensor\n",
        "\n",
        "def compute_spectral_norm(conv, u_init=None, im_size=(3, 32, 32), k=1):\n",
        "    if u_init is None:\n",
        "        with torch.no_grad():\n",
        "            u_init = torch.randn(1, *im_size).to(conv.weight.device)\n",
        "    u_init = u_init.to(conv.weight.device)\n",
        "    with torch.no_grad():\n",
        "        return power_method(lambda u: torch.nn.functional.conv2d(u, conv.weight, padding=tuple(v//2 for v in conv.weight.shape[2:])),\n",
        "                lambda v: torch.nn.functional.conv_transpose2d(v, conv.weight, padding=tuple(v//2 for v in conv.weight.shape[2:])),\n",
        "                u_init, k)\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    convo = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1,bias=False)\n",
        "    layers = [convo]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBRVEACgBwFI"
      },
      "outputs": [],
      "source": [
        "# Define activation functions\n",
        "\n",
        "def LeakyReLU(x,slope=0.01,right_slope=1):\n",
        "  return torch.max(slope*x, right_slope*x)\n",
        "\n",
        "def ReLU(x,slope=0.01):\n",
        "  return LeakyReLU(x,slope=0)\n",
        "\n",
        "def Softplus(x,slope=0.01):\n",
        "  return torch.log(1+torch.exp(x))\n",
        "\n",
        "def LeakySoftplus(x, slope=0.01):\n",
        "  return slope*x+ torch.log(1+torch.exp(x)) - 0.693147207\n",
        "\n",
        "def SemiLeakySoftplus(x, slope=0.01):\n",
        "  return torch.where(x < 0, slope * x + torch.log(1 + torch.exp(x)), torch.log(1 + torch.exp(x))) - 0.693147207\n",
        "\n",
        "def Tanh(x,slope=0.01):\n",
        "  return torch.tanh(x)\n",
        "\n",
        "def LeakyTanh(x, slope=0.01):\n",
        "  return slope*x + Tanh(x)\n",
        "\n",
        "def Sigmoid(x,slope=0.01):\n",
        "  return 1/(1+torch.exp(-x))\n",
        "\n",
        "def LeakySigmoid(x, slope=0.01):\n",
        "  return slope*x + Sigmoid(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFF28iniefX7"
      },
      "outputs": [],
      "source": [
        "# Functions used to generate datasets\n",
        "\n",
        "def GenerateCircleDataset(noise=False, num_points=num_points):\n",
        "  # Generate random points\n",
        "  points = np.random.randn(num_points, dimension)\n",
        "\n",
        "  # Calculate distances to the origin\n",
        "  distances = np.linalg.norm(points, axis=1)\n",
        "\n",
        "  # Generate labels: 0 if distance < 2/3, 1 if 2/3 < distance < 4/3, 2 if distance > 4/3\n",
        "  labels = np.zeros_like(distances)\n",
        "  labels[distances > 2/3] = 1\n",
        "  labels[distances > 4/3] = 2\n",
        "\n",
        "  # Convert numpy arrays to PyTorch tensors\n",
        "  labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "  if noise:\n",
        "    noise = 0.1*np.random.randn(num_points, dimension)\n",
        "    noisy_points = points + noise\n",
        "    points_tensor = torch.tensor(noisy_points, dtype=torch.float32)\n",
        "  else:\n",
        "    points_tensor = torch.tensor(points, dtype=torch.float32)\n",
        "\n",
        "  return points_tensor, labels_tensor\n",
        "\n",
        "def GenerateSquareDataset(noise=False, num_points=num_points):\n",
        "  # Generate random points uniformly between -1 and 1\n",
        "  points = np.random.uniform(low=-1.0, high=1.0, size=(num_points, dimension))\n",
        "\n",
        "  # Initialize labels\n",
        "  labels = np.zeros(num_points)\n",
        "\n",
        "  # Assign labels based on quadrant\n",
        "  for i, point in enumerate(points):\n",
        "      if point[0] >= 0 and point[1] >= 0:\n",
        "          labels[i] = 0  # Quadrant 1\n",
        "      elif point[0] < 0 and point[1] >= 0:\n",
        "          labels[i] = 1  # Quadrant 2\n",
        "      elif point[0] < 0 and point[1] < 0:\n",
        "          labels[i] = 2  # Quadrant 3\n",
        "      else:\n",
        "          labels[i] = 3  # Quadrant 4\n",
        "\n",
        "  # Convert numpy arrays to PyTorch tensors\n",
        "  labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "  if noise:\n",
        "      # Add noise to the points\n",
        "      noise = 0.1 * np.random.randn(num_points, dimension)\n",
        "      noisy_points = points + noise\n",
        "      points_tensor = torch.tensor(noisy_points, dtype=torch.float32)\n",
        "  else:\n",
        "      points_tensor = torch.tensor(points, dtype=torch.float32)\n",
        "\n",
        "  return points_tensor, labels_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg-fjm-rHDoc"
      },
      "outputs": [],
      "source": [
        "class multiClassHingeLoss(nn.Module): # From the paper Dynamical Systems\n",
        "    def __init__(self, p=1, margin=margin, device='cpu', size_average=True):\n",
        "        super(multiClassHingeLoss, self).__init__()\n",
        "        self.margin=margin\n",
        "        self.size_average=size_average\n",
        "        self.device = device\n",
        "    def forward(self, output, y):\n",
        "        output_y=output[torch.arange(0,y.size()[0]).long().to(self.device),y.data.to(self.device)].view(-1,1) #it is a (Batch Size x 1) tensor, having entries that are x[y]\n",
        "        loss=output-output_y+self.margin #this has self.margin in position y and the difference between the entry of x and x[y] in the other positions\n",
        "        #remove i=y items\n",
        "        loss[torch.arange(0,y.size()[0]).long().to(self.device),y.data.to(self.device)]=0 #sets to 0 the entry in position y, instead of having self.margin\n",
        "        #max(0,_)\n",
        "        loss[loss<0]=0 #sets to 0 the entries of loss where we have negative numbers, i.e. those meeting the margin (there is a higher difference than the margin between x[y] and x[i])\n",
        "        #sum up\n",
        "        loss=torch.sum(loss)\n",
        "        if(self.size_average):\n",
        "            loss/=output.size()[0]\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff4Jg0d7dPkL"
      },
      "outputs": [],
      "source": [
        "if dataset_type == 'square':\n",
        "  out_channels = 4\n",
        "elif dataset_type == 'circle':\n",
        "  out_channels = 3\n",
        "else:\n",
        "  raise ValueError(\"Invalid dataset type\")\n",
        "\n",
        "class ExpansiveContractiveNet(nn.Module): # implemented using fully connected layers\n",
        "    def __init__(self, nlayers, nl, in_channels=dimension, sub_steps=1):\n",
        "        super(ExpansiveContractiveNet, self).__init__()\n",
        "        self.nlayers = nlayers # number of layers\n",
        "        self.nl = nl  # activation function\n",
        "        self.slope = slope\n",
        "        self.sub_steps = sub_steps\n",
        "        self.u = torch.nn.Parameter(torch.rand(self.nlayers))\n",
        "        self.rescalings = nn.Parameter(torch.rand(1+self.nlayers//2))\n",
        "\n",
        "        # Layers\n",
        "        self.lift = nn.Linear(in_channels, nodes_per_layer)\n",
        "\n",
        "        self.inner_layers = nn.ModuleList([nn.Linear(nodes_per_layer, nodes_per_layer) for i in range(self.nlayers)])\n",
        "\n",
        "        self.project = nn.Linear(nodes_per_layer, out_channels)\n",
        "\n",
        "        # for i in range(self.nlayers):\n",
        "        #     makeDeltaOrthogonal(self.inner_layers[i].weight.data, nn.init.calculate_gain('leaky_relu',self.slope))\n",
        "\n",
        "        self.rescalings = nn.Parameter(torch.rand(1+self.nlayers//2))\n",
        "\n",
        "    def getDepth(self):\n",
        "        return self.nlayers\n",
        "\n",
        "    def getReg(self,):\n",
        "        reg = 0\n",
        "        for i in range(self.nlayers):\n",
        "            reg += deconv_orth_dist_2d(self.inner_layers[i].weight)\n",
        "        return reg\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.rescalings.data[:-1] = torch.clip(self.rescalings.data[:-1],min=0,max=1)\n",
        "        x = torch.relu(self.rescalings[-1] * self.lift(x))\n",
        "\n",
        "        dts = Positive(self.u)\n",
        "\n",
        "        for i in np.arange(0,self.nlayers,2):\n",
        "\n",
        "          dte = dts[i]/self.sub_steps\n",
        "          dtc = dts[i+1]/self.sub_steps\n",
        "          Ae = self.inner_layers[i]\n",
        "          Ac = self.inner_layers[i+1]\n",
        "\n",
        "          for k in range(self.sub_steps):\n",
        "                x = x + dte * self.rescalings[i//2] * self.nl(Ae(x), self.slope)\n",
        "                x = x - dtc * self.nl(Ac(x), self.slope)\n",
        "\n",
        "        x = self.project(x) # project to number of labels\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BRwPeDlrNdQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFe600t60LQC"
      },
      "outputs": [],
      "source": [
        "def TrainAndAttack(model, batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy0, robust_accuracy0, test_points_tensor, test_labels_tensor, patience=6, device=device,epochs=epochs,reg=True,slope=slope,constrain = True, margin=margin):\n",
        "    criterion = multiClassHingeLoss(margin=margin)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    train_points, val_points, train_labels, val_labels = train_test_split(points_tensor, labels_tensor, test_size=0.2)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    gamma = .1 # used to scale the orthogonal constraint regulation\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        for i in range(0, len(train_points), batch_size):\n",
        "          inputs = train_points[i:i+batch_size]\n",
        "          targets = train_labels[i:i+batch_size]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          if reg:\n",
        "                loss += gamma * model.getReg()\n",
        "          if math.isnan(loss.item()) or math.isinf(loss.item()):\n",
        "              print(\"NaN loss detected, stopping training.\")\n",
        "              return epoch, -1\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "          if constrain:\n",
        "                with torch.no_grad():\n",
        "                    refLip = 1\n",
        "\n",
        "                    lift_norm = torch.linalg.matrix_norm(model.lift.weight.data,2)\n",
        "                    model.lift.weight.data /= max(1,lift_norm/refLip)\n",
        "\n",
        "                for q in range(model.getDepth()):\n",
        "                  norm = torch.linalg.matrix_norm(model.inner_layers[q].weight.data,2)\n",
        "                  model.inner_layers[q].weight.data /= max(1,norm/refLip)\n",
        "\n",
        "        # Compute accuracy on test set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            norms = []\n",
        "            nonort = []\n",
        "            lift = []\n",
        "            liftOrt = []\n",
        "\n",
        "            n = torch.linalg.matrix_norm(model.lift.weight.data,2)\n",
        "            lift.append(n.item())\n",
        "\n",
        "            for s in range(model.getDepth()):\n",
        "              n = torch.linalg.matrix_norm(model.inner_layers[s].weight.data,2)\n",
        "              nonort.append(orth_dist(model.inner_layers[s].weight).item())\n",
        "              norms.append(n.item())\n",
        "\n",
        "            outputs = model(test_points_tensor)\n",
        "            _, predicted = torch.max(outputs, 1) # Maximum along axis=1\n",
        "            accuracy = (predicted == test_labels_tensor).float().mean()\n",
        "\n",
        "        # Validation step\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(val_points), batch_size):\n",
        "                inputs = val_points[i:i+batch_size]\n",
        "                targets = val_labels[i:i+batch_size]\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                if torch.isnan(loss) or math.isinf(loss.item()):\n",
        "                  print(\"NaN loss detected, stopping training.\")\n",
        "                  return epoch, -1\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_points) // batch_size\n",
        "        print(f'Time: {datetime.now().strftime(\"%H:%M:%S\")}, Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}, Val Loss: {val_loss}, Loop: {loop+1} / {num_loops}, Epochs without improvement:{epochs_without_improvement}')\n",
        "\n",
        "        # Check for early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "                break\n",
        "        # scheduler.step()\n",
        "    # print('Training Done')\n",
        "    accuracy0[loop] = accuracy\n",
        "    print(f'Accuracy: {accuracy}, Loop: {loop}')\n",
        "\n",
        "    model.eval()\n",
        "    fmodel = fb.PyTorchModel(model, bounds=(-30, 30), preprocessing=None)\n",
        "\n",
        "    # Create the Foolbox attack\n",
        "    attack = fb.attacks.LinfDeepFoolAttack()\n",
        "\n",
        "    # Evaluate the adversarial robustness\n",
        "    num_adversarial = 0\n",
        "    attack = fb.attacks.LinfDeepFoolAttack()\n",
        "    raw, clipped, is_adv = attack(fmodel, points_tensor, labels_tensor, epsilons=epsilons)\n",
        "    robust_accuracy0[loop] = torch.mean((1-1.*is_adv),axis=1).detach().cpu().numpy()\n",
        "\n",
        "    return epoch, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9SAJXrwVIeE3"
      },
      "outputs": [],
      "source": [
        "# Initialize arrays\n",
        "number_of_loops = num_loops\n",
        "accuracy1 = np.zeros(number_of_loops)\n",
        "accuracy2 = np.zeros(number_of_loops)\n",
        "accuracy3 = np.zeros(number_of_loops)\n",
        "accuracy4 = np.zeros(number_of_loops)\n",
        "accuracy5 = np.zeros(number_of_loops)\n",
        "accuracy6 = np.zeros(number_of_loops)\n",
        "accuracy7 = np.zeros(number_of_loops)\n",
        "accuracy8 = np.zeros(number_of_loops)\n",
        "accuracy9 = np.zeros(number_of_loops)\n",
        "\n",
        "actual_epochs = np.zeros([number_of_loops, 9])\n",
        "successes = np.zeros([number_of_loops, 9])\n",
        "\n",
        "\n",
        "robust_accuracy1 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy2 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy3 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy4 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy5 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy6 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy7 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy8 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "robust_accuracy9 = np.zeros((number_of_loops,number_of_epsilons))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for loop in range(num_loops):\n",
        "\n",
        "  # Generate datasets\n",
        "  points_tensor, labels_tensor = GenerateSquareDataset(noise=noise,num_points=num_points)\n",
        "  test_points_tensor, test_labels_tensor = GenerateSquareDataset(noise=False, num_points=num_test_points)\n",
        "\n",
        "  # Instantiate models\n",
        "  model1 = ExpansiveContractiveNet(inner_layers, nl=ReLU)\n",
        "  model2 = ExpansiveContractiveNet(inner_layers, nl=LeakyReLU)\n",
        "  model3 = ExpansiveContractiveNet(inner_layers, nl=Softplus)\n",
        "  model4 = ExpansiveContractiveNet(inner_layers, nl=LeakySoftplus)\n",
        "  model5 = ExpansiveContractiveNet(inner_layers, nl=SemiLeakySoftplus)\n",
        "  model6 = ExpansiveContractiveNet(inner_layers, nl=Tanh)\n",
        "  model7 = ExpansiveContractiveNet(inner_layers, nl=LeakyTanh)\n",
        "  model8 = ExpansiveContractiveNet(inner_layers, nl=Sigmoid)\n",
        "  model9 = ExpansiveContractiveNet(inner_layers, nl=LeakySigmoid)\n",
        "\n",
        "  # Train and attack the models\n",
        "  actual_epochs[loop, 0], successes[loop,0] = TrainAndAttack(model1,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy1, robust_accuracy1, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 1], successes[loop,1] = TrainAndAttack(model2,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy2, robust_accuracy2, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 2], successes[loop,2] = TrainAndAttack(model3,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy3, robust_accuracy3, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 3], successes[loop,3] = TrainAndAttack(model4,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy4, robust_accuracy4, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 4], successes[loop,4] = TrainAndAttack(model5,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy5, robust_accuracy5, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 5], successes[loop,5] = TrainAndAttack(model6,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy6, robust_accuracy6, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 6], successes[loop,6] = TrainAndAttack(model7,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy7, robust_accuracy7, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 7], successes[loop,7] = TrainAndAttack(model8,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy8, robust_accuracy8, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  actual_epochs[loop, 8], successes[loop,8] = TrainAndAttack(model9,batch_size, number_of_loops, number_of_epsilons, points_tensor, labels_tensor, accuracy9, robust_accuracy9, test_points_tensor,test_labels_tensor)\n",
        "\n",
        "  # Visualize the last dataset\n",
        "  if loop == number_of_loops-1 :\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(points_tensor[:, 0], points_tensor[:, 1], c=labels_tensor, cmap='viridis')\n",
        "    plt.colorbar(label='Labels')\n",
        "    plt.title('Dataset')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "78zDCrb_39fO"
      },
      "outputs": [],
      "source": [
        "actual_epochs = np.transpose(actual_epochs)\n",
        "successes = np.transpose(successes)\n",
        "\n",
        "print('Successes, row = model, collumn = loop:')\n",
        "print(successes)\n",
        "\n",
        "\n",
        "train_accuracies = [\n",
        "    accuracy1, accuracy2, accuracy3,\n",
        "    accuracy4, accuracy5, accuracy6,\n",
        "    accuracy7, accuracy8, accuracy9\n",
        "]\n",
        "train_accuracies = np.array(train_accuracies)\n",
        "\n",
        "robust_accuracies = [\n",
        "    robust_accuracy1, robust_accuracy2, robust_accuracy3,\n",
        "    robust_accuracy4, robust_accuracy5, robust_accuracy6,\n",
        "    robust_accuracy7, robust_accuracy8, robust_accuracy9\n",
        "]\n",
        "\n",
        "# Filter the accuracies to exclude failed runs\n",
        "marked_train_accuracies = np.where(successes == 0, train_accuracies, np.nan)\n",
        "average_successful_train_accuracies = np.nanmean(marked_train_accuracies, axis=1)\n",
        "\n",
        "# Filter the robust accuracies to exclude failed runs\n",
        "marked_robust_accuracies = robust_accuracies\n",
        "marked_robust_accuracies = np.array(marked_robust_accuracies)\n",
        "for j in range(number_of_loops):\n",
        "  for i in range(9):\n",
        "    if successes[i,j] == -1:\n",
        "      marked_robust_accuracies[i,j] = np.nan\n",
        "\n",
        "average_successful_robust_accuracies = np.zeros_like(average_successful_train_accuracies)\n",
        "for i in range(9):\n",
        "  average_successful_robust_accuracies[i] = np.nanmean(marked_robust_accuracies[i,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T3VS5FRmJ6hz"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of fails per model\n",
        "fails_per_model = np.sum(successes == -1, axis=1)\n",
        "\n",
        "# Calculate the average number of required epochs per model (excluding failed ones)\n",
        "avg_epochs_per_model = np.where(successes != -1, actual_epochs, np.nan)\n",
        "avg_epochs_per_model = np.nanmean(avg_epochs_per_model, axis=1)\n",
        "\n",
        "# Plot the total number of fails per model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(model_names, fails_per_model, color='red')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Total Number of Fails')\n",
        "plt.title('Total Number of Fails per Model')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the average number of required epochs per model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(model_names, avg_epochs_per_model, color='blue')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Average Number of Required Epochs')\n",
        "plt.title('Average Number of Required Epochs per Model')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9Mo-2lbsKEJw"
      },
      "outputs": [],
      "source": [
        "# Plot accuracies\n",
        "n_models = len(average_successful_train_accuracies)  # Number of models\n",
        "ind = np.arange(n_models)  # the x locations for the groups\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "# Plotting standard accuracies\n",
        "fig1, ax1 = plt.subplots()\n",
        "train_bars = ax1.bar(ind, average_successful_train_accuracies, width, label='Standard', color='b')\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax1.set_xlabel('Models')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Standard Accuracy by Model')\n",
        "ax1.set_xticks(ind)\n",
        "ax1.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
        "\n",
        "def autolabel(bars, ax):\n",
        "    \"\"\"Attach a text label above each bar in *bars*, displaying its height.\"\"\"\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate('{}'.format(round(height, 4)),\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(train_bars, ax1)\n",
        "\n",
        "fig1.tight_layout()\n",
        "\n",
        "# Plotting robust accuracies\n",
        "fig2, ax2 = plt.subplots()\n",
        "robust_bars = ax2.bar(ind, average_successful_robust_accuracies, width, label='Robust', color='r')\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax2.set_xlabel('Models')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Robust Accuracy by Model')\n",
        "ax2.set_xticks(ind)\n",
        "ax2.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
        "\n",
        "autolabel(robust_bars, ax2)\n",
        "\n",
        "fig2.tight_layout()\n",
        "\n",
        "# Show both plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yP3HTK02KF5K"
      },
      "outputs": [],
      "source": [
        "# Filter the robust accuracies to exclude failed runs\n",
        "average_epsilon_attacks = np.zeros([9, number_of_epsilons])\n",
        "for i in range(9):\n",
        "  for j in range(number_of_loops):\n",
        "    average_epsilon_attacks[i] = np.nanmean(marked_robust_accuracies[i,:,:], axis=0)\n",
        "\n",
        "\n",
        "accuracies = [\n",
        "    robust_accuracy1.mean(axis=0), robust_accuracy2.mean(axis=0), robust_accuracy3.mean(axis=0),\n",
        "    robust_accuracy4.mean(axis=0), robust_accuracy5.mean(axis=0), robust_accuracy6.mean(axis=0),\n",
        "    robust_accuracy7.mean(axis=0), robust_accuracy8.mean(axis=0), robust_accuracy9.mean(axis=0)\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot each line with corresponding color\n",
        "for i, array in enumerate(average_epsilon_attacks):\n",
        "    plt.plot(epsilons, array, marker='.', color=colors[i], label=model_names[i])\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Epsilon')\n",
        "plt.ylabel('Robust Accuracy')\n",
        "plt.title('Robust Accuracies by Model and Epsilon')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2QYnf3g5Kkiw"
      },
      "outputs": [],
      "source": [
        "# Robust accuracy per model + variance and minina and maxima\n",
        "maxima = np.zeros([9, number_of_epsilons])\n",
        "minima = np.zeros([9, number_of_epsilons])\n",
        "\n",
        "for i in range(9):\n",
        "  maxima[i] = np.nanmax(marked_robust_accuracies[i,:,:],axis=0)\n",
        "  minima[i] = np.nanmin(marked_robust_accuracies[i,:,:],axis=0)\n",
        "\n",
        "n_models = len(marked_robust_accuracies)\n",
        "var_accuracies = np.zeros((n_models, len(epsilons)))\n",
        "\n",
        "# Plot each line with error bars for mean, max, and min accuracies\n",
        "for i, array in enumerate(average_epsilon_attacks):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    var_accuracies[i] = np.nanstd(marked_robust_accuracies[i], axis=0)\n",
        "\n",
        "    plt.plot(epsilons, array, label='Mean', color=colors[i])\n",
        "    plt.plot(epsilons, maxima[i], label='Maximum', color=colors[i])\n",
        "    plt.plot(epsilons, minima[i], label='Minimum', color=colors[i])\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.xlabel('Epsilon')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Robust accuracies by epsilon - ' + model_names[i])\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the variation on robust accuracies\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, array in enumerate(var_accuracies):\n",
        "    plt.plot(epsilons, array, marker='x', label=model_names[i], color=colors[i])\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Epsilon')\n",
        "plt.ylabel('Standard deviation of Robust Accuracy')\n",
        "plt.title('Standard deviation of Robust Accuracies by Model and Epsilon')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}